import taskManager from '../services/taskManager.js';
import { PagesJaunesScraper } from '../services/scrapers/pagesJaunesScraper.js';
import { getGoogleMapsService } from '../services/googleMapsService.js';
import { getLinkedInScraper } from '../services/scrapers/linkedInScraper.js';
import { Prospect, Tag, SourceScraping } from '../models/index.js';
import { Op } from 'sequelize';
import { addressesMatch } from '../utils/addressNormalizer.js';

/**
 * @route   POST /api/scraping/lancer
 * @desc    Lancer une tÃ¢che de scraping asynchrone
 * @access  Public
 */
export const lancerScraping = async (req, res) => {
  try {
    const { keyword, location, source = 'Pages Jaunes', startPage = 1, maxPages = 1, maxResults = 10, excludeDuplicates = false } = req.body;

    // Validation des paramÃ¨tres
    if (!keyword || !location) {
      return res.status(400).json({
        error: 'Bad Request',
        message: 'Les paramÃ¨tres "keyword" et "location" sont requis',
      });
    }

    // CrÃ©er une tÃ¢che
    const task = taskManager.createTask({
      keyword,
      location,
      source,
      startPage,
      maxPages,
      maxResults,
      excludeDuplicates,
    });

    // Lancer le scraping de maniÃ¨re asynchrone
    scrapeAsync(task.id, keyword, location, { startPage, maxPages, maxResults, source, excludeDuplicates });

    // Retourner immÃ©diatement l'ID de la tÃ¢che
    res.status(202).json({
      task_id: task.id,
      status: task.status,
      message: 'TÃ¢che de scraping crÃ©Ã©e et lancÃ©e',
      params: {
        keyword,
        location,
        source,
        maxPages,
        maxResults,
      },
    });
  } catch (error) {
    console.error('Error launching scraping:', error);
    res.status(500).json({
      error: 'Internal Server Error',
      message: error.message,
    });
  }
};

/**
 * Fonction asynchrone pour effectuer le scraping
 * @param {string} taskId - ID de la tÃ¢che
 * @param {string} keyword - Mot-clÃ© de recherche
 * @param {string} location - Localisation
 * @param {Object} options - Options de scraping
 */
async function scrapeAsync(taskId, keyword, location, options = {}) {
  try {
    // Mettre Ã  jour le statut Ã  "in_progress"
    taskManager.updateTaskStatus(taskId, 'in_progress');

    const { source = 'Pages Jaunes', startPage = 1, maxPages = 1, maxResults = 10, excludeDuplicates = false } = options;
    let prospects = [];
    let scrapingResult = null; // Stocker le rÃ©sultat complet du scraping

    // CrÃ©er une fonction de vÃ©rification de doublons en temps rÃ©el
    const isDuplicate = excludeDuplicates ? async (prospectData) => {
      // VÃ©rifier d'abord par email ou URL (exact match)
      const existingExact = await Prospect.findOne({
        where: {
          [Op.or]: [
            prospectData.email ? { email: prospectData.email } : null,
            prospectData.url_site ? { url_site: prospectData.url_site } : null,
          ].filter(Boolean),
        },
      });

      if (existingExact) return true;

      // VÃ©rifier par nom + adresse avec normalisation
      if (prospectData.nom_entreprise && prospectData.adresse) {
        const potentialDuplicates = await Prospect.findAll({
          where: {
            nom_entreprise: prospectData.nom_entreprise
          }
        });

        // Comparer les adresses normalisÃ©es
        for (const candidate of potentialDuplicates) {
          if (candidate.adresse && addressesMatch(prospectData.adresse, candidate.adresse)) {
            return true;
          }
        }
      }

      return false;
    } : null;

    // Choisir le scraper appropriÃ© selon la source
    if (source === 'Google Maps') {
      // Utiliser le service Google Maps (scraper ou API selon config)
      const googleMapsService = getGoogleMapsService();

      prospects = await googleMapsService.search(
        { keyword, location, maxResults },
        (progress, message) => {
          taskManager.updateTaskProgress(taskId, progress, { message });
        }
      );

    } else if (source === 'LinkedIn') {
      // LinkedIn (Mode Public - LimitÃ©)
      const linkedInScraper = getLinkedInScraper();

      console.log(`[ScrapingController] ðŸ“Š LinkedIn scraping - Mode Public`);
      console.log(`[ScrapingController] âš ï¸ Limites : Max ${Math.min(maxResults, 10)} profils, dÃ©lais longs`);

      prospects = await linkedInScraper.scrape(keyword, location, {
        maxResults: Math.min(maxResults, 10), // Forcer limite de 10 max
        onProgress: (data) => {
          taskManager.updateTaskProgress(taskId, data.progress, {
            message: data.message,
            step: data.step
          });
        },
      });

      // LinkedIn retourne directement un tableau de prospects (pas de result wrapper)
      // Pas besoin de transformation

    } else {
      // Pages Jaunes (par dÃ©faut)
      const scraper = new PagesJaunesScraper();

      console.log(`[ScrapingController] Mode excludeDuplicates: ${excludeDuplicates ? 'OUI' : 'NON'}`);

      scrapingResult = await scraper.scrape(keyword, location, {
        startPage,
        maxPages,
        maxResults,
        excludeDuplicates,
        isDuplicate, // Callback pour vÃ©rifier les doublons en temps rÃ©el
        onProgress: (progress, data) => {
          taskManager.updateTaskProgress(taskId, progress, data);
        },
      });

      prospects = scrapingResult.prospects;
    }

    // Sauvegarder les prospects en base de donnÃ©es avec leur source
    const savedProspects = await saveProspects(prospects, keyword, source);

    // PrÃ©parer les donnÃ©es de rÃ©sultat selon la source
    let taskResult = {
      prospects: savedProspects,
      total: savedProspects.length,
      duplicates_skipped: prospects.length - savedProspects.length,
    };

    // Ajouter des mÃ©triques spÃ©cifiques selon la source
    if (source === 'LinkedIn') {
      taskResult.source = 'LinkedIn (Mode Public)';
      taskResult.captcha_detected = getLinkedInScraper().captchaDetected;
      taskResult.success = savedProspects.length > 0;
    } else if (source === 'Google Maps') {
      taskResult.source = 'Google Maps';
      taskResult.success = true;
    } else {
      // Pages Jaunes
      taskResult.pages_scraped = scrapingResult?.pages_scraped || 1;
      taskResult.success = scrapingResult?.success || false;
    }

    // Marquer la tÃ¢che comme terminÃ©e
    taskManager.completeTask(taskId, taskResult);

    console.log(`[ScrapingController] TÃ¢che ${taskId} terminÃ©e: ${savedProspects.length} prospects sauvegardÃ©s`);
  } catch (error) {
    console.error(`[ScrapingController] Erreur tÃ¢che ${taskId}:`, error);
    taskManager.failTask(taskId, error);
  }
}

/**
 * Sauvegarder les prospects en base de donnÃ©es avec gestion des sources multiples
 * GÃ¨re les doublons et l'enrichissement automatique
 * @param {Array} prospects - Liste des prospects Ã  sauvegarder
 * @param {string} keyword - Mot-clÃ© pour crÃ©er un tag
 * @param {string} sourceName - Nom de la source de scraping (ex: 'Google Maps', 'Pages Jaunes')
 * @returns {Array} Prospects sauvegardÃ©s
 */
async function saveProspects(prospects, keyword, sourceName) {
  const savedProspects = [];

  // CrÃ©er ou rÃ©cupÃ©rer le tag basÃ© sur le keyword
  const [tag] = await Tag.findOrCreate({
    where: { nom: keyword.charAt(0).toUpperCase() + keyword.slice(1) },
  });

  // RÃ©cupÃ©rer ou crÃ©er la source de scraping
  const [source] = await SourceScraping.findOrCreate({
    where: { nom: sourceName },
    defaults: {
      description: `Source de scraping: ${sourceName}`,
      actif: true,
    }
  });

  console.log(`[ScrapingController] ðŸ“Œ Source utilisÃ©e: ${source.nom} (ID: ${source.id})`);

  for (const prospectData of prospects) {
    try {
      // Ã‰tape 1 : VÃ©rifier d'abord les doublons exacts (email, URL, GPS)
      let existingProspect = await Prospect.findOne({
        where: {
          [Op.or]: [
            // MÃªme email
            prospectData.email ? { email: prospectData.email } : null,
            // MÃªme URL
            prospectData.url_site ? { url_site: prospectData.url_site } : null,
            // MÃªme nom ET mÃªmes coordonnÃ©es GPS
            prospectData.nom_entreprise && prospectData.latitude && prospectData.longitude ? {
              nom_entreprise: prospectData.nom_entreprise,
              latitude: prospectData.latitude,
              longitude: prospectData.longitude
            } : null,
          ].filter(Boolean),
        },
        include: [
          { model: Tag, as: 'tags' },
          { model: SourceScraping, as: 'sources' }
        ]
      });

      // Ã‰tape 2 : Si pas trouvÃ©, vÃ©rifier par nom + adresse normalisÃ©e
      if (!existingProspect && prospectData.nom_entreprise && prospectData.adresse) {
        const potentialDuplicates = await Prospect.findAll({
          where: { nom_entreprise: prospectData.nom_entreprise },
          include: [
            { model: Tag, as: 'tags' },
            { model: SourceScraping, as: 'sources' }
          ]
        });

        // Comparer les adresses normalisÃ©es
        for (const candidate of potentialDuplicates) {
          if (candidate.adresse && addressesMatch(prospectData.adresse, candidate.adresse)) {
            existingProspect = candidate;
            console.log(`[ScrapingController] ðŸ” Doublon dÃ©tectÃ© via normalisation d'adresse:`);
            console.log(`   - Base: "${candidate.adresse}"`);
            console.log(`   - Nouveau: "${prospectData.adresse}"`);
            break;
          }
        }
      }

      if (existingProspect) {
        console.log(`[ScrapingController] âš ï¸  Doublon dÃ©tectÃ©: ${prospectData.nom_entreprise} (${prospectData.adresse || 'pas d\'adresse'})`);

        // VÃ©rifier si cette source est dÃ©jÃ  associÃ©e
        const hasSource = existingProspect.sources.some(s => s.id === source.id);
        if (!hasSource) {
          await existingProspect.addSource(source);
          console.log(`[ScrapingController] âœ… Source "${source.nom}" ajoutÃ©e au prospect existant`);
        } else {
          console.log(`[ScrapingController] â„¹ï¸  Source "${source.nom}" dÃ©jÃ  associÃ©e Ã  ce prospect`);
        }

        // Enrichir les donnÃ©es existantes au lieu de skip
        const updatedFields = {};
        let hasUpdates = false;

        // Champs Ã  enrichir uniquement si null/vide (donnÃ©es stables)
        const fieldsToEnrichIfNull = [
          'adresse', 'latitude', 'longitude', 'ville', 'code_postal'
        ];

        // Champs Ã  toujours mettre Ã  jour si diffÃ©rents (donnÃ©es qui peuvent changer)
        const fieldsToAlwaysUpdate = ['nom_contact', 'email', 'telephone', 'url_site', 'note'];

        // 1. Enrichir les champs null/vides
        fieldsToEnrichIfNull.forEach(field => {
          const existingValue = existingProspect[field];
          const newValue = prospectData[field];

          if ((existingValue === null || existingValue === undefined || existingValue === '') &&
              newValue !== null && newValue !== undefined && newValue !== '') {
            updatedFields[field] = newValue;
            hasUpdates = true;
          }
        });

        // 2. Mettre Ã  jour les champs qui peuvent changer (si la nouvelle valeur est diffÃ©rente et non vide)
        fieldsToAlwaysUpdate.forEach(field => {
          const existingValue = existingProspect[field];
          const newValue = prospectData[field];

          // Mettre Ã  jour si:
          // 1. La nouvelle valeur n'est pas null/undefined/vide
          // 2. ET la nouvelle valeur est diffÃ©rente de l'existante
          if (newValue !== null && newValue !== undefined && newValue !== '' &&
              existingValue !== newValue) {
            updatedFields[field] = newValue;
            hasUpdates = true;
            console.log(`[ScrapingController] ðŸ”„ Mise Ã  jour de ${field}: "${existingValue}" â†’ "${newValue}"`);
          }
        });

        if (hasUpdates) {
          await existingProspect.update(updatedFields);
          console.log(`[ScrapingController] âœ… DonnÃ©es enrichies: ${Object.keys(updatedFields).join(', ')}`);
        } else {
          console.log(`[ScrapingController] â„¹ï¸  Aucune nouvelle donnÃ©e Ã  enrichir`);
        }

        // Ajouter le tag si pas dÃ©jÃ  prÃ©sent
        const hasTag = existingProspect.tags.some(t => t.id === tag.id);
        if (!hasTag) {
          await existingProspect.addTag(tag);
          console.log(`[ScrapingController] âœ… Tag "${tag.nom}" ajoutÃ© au prospect existant`);
        }

        // Recharger avec les relations pour le retour
        await existingProspect.reload({
          include: [
            { model: Tag, as: 'tags' },
            { model: SourceScraping, as: 'sources' }
          ],
        });

        savedProspects.push(existingProspect);
        continue; // Passer au prospect suivant
      }

      // CrÃ©er le nouveau prospect (sans source_scraping dans les champs)
      const prospect = await Prospect.create({
        nom_entreprise: prospectData.nom_entreprise,
        nom_contact: prospectData.nom_contact || null,
        email: prospectData.email || null,
        telephone: prospectData.telephone || null,
        adresse: prospectData.adresse || null,
        url_site: prospectData.url_site || null,
        latitude: prospectData.latitude || null,
        longitude: prospectData.longitude || null,
        note: prospectData.note || null,
        ville: prospectData.ville || null,
        code_postal: prospectData.code_postal || null,
      });

      // Associer la source de scraping
      await prospect.addSource(source);
      console.log(`[ScrapingController] âœ… Source "${source.nom}" associÃ©e au nouveau prospect`);

      // Associer le tag
      await prospect.addTag(tag);

      // Recharger avec les relations pour le retour
      await prospect.reload({
        include: [
          { model: Tag, as: 'tags' },
          { model: SourceScraping, as: 'sources' }
        ],
      });

      savedProspects.push(prospect);
      console.log(`[ScrapingController] âœ… Nouveau prospect sauvegardÃ©: ${prospect.nom_entreprise}`);
    } catch (error) {
      console.error(`[ScrapingController] âŒ Erreur sauvegarde prospect:`, error);
    }
  }

  return savedProspects;
}

/**
 * @route   GET /api/scraping/status/:task_id
 * @desc    RÃ©cupÃ©rer le statut d'une tÃ¢che de scraping
 * @access  Public
 */
export const getScrapingStatus = async (req, res) => {
  try {
    const { task_id } = req.params;

    const task = taskManager.getTask(task_id);

    if (!task) {
      return res.status(404).json({
        error: 'Not Found',
        message: `TÃ¢che non trouvÃ©e: ${task_id}`,
      });
    }

    res.json({
      task_id: task.id,
      status: task.status,
      progress: task.progress,
      params: task.params,
      results: {
        total: task.results.total,
        pages_scraped: task.results.pages_scraped,
        errors: task.results.errors,
      },
      createdAt: task.createdAt,
      startedAt: task.startedAt,
      completedAt: task.completedAt,
      error: task.error,
    });
  } catch (error) {
    console.error('Error getting scraping status:', error);
    res.status(500).json({
      error: 'Internal Server Error',
      message: error.message,
    });
  }
};

/**
 * @route   POST /api/scraping/cancel/:task_id
 * @desc    Annuler une tÃ¢che de scraping en cours
 * @access  Public
 */
export const cancelScraping = async (req, res) => {
  try {
    const { task_id } = req.params;

    const cancelled = taskManager.cancelTask(task_id);

    if (!cancelled) {
      return res.status(400).json({
        error: 'Bad Request',
        message: 'Impossible d\'annuler cette tÃ¢che (non trouvÃ©e ou dÃ©jÃ  terminÃ©e)',
      });
    }

    res.json({
      task_id,
      status: 'cancelled',
      message: 'TÃ¢che annulÃ©e avec succÃ¨s',
    });
  } catch (error) {
    console.error('Error cancelling scraping:', error);
    res.status(500).json({
      error: 'Internal Server Error',
      message: error.message,
    });
  }
};

/**
 * @route   GET /api/scraping/tasks
 * @desc    RÃ©cupÃ©rer toutes les tÃ¢ches de scraping
 * @access  Public
 */
export const getAllTasks = async (req, res) => {
  try {
    const { status, limit = 20 } = req.query;

    const tasks = taskManager.getAllTasks({
      status,
      limit: parseInt(limit),
    });

    res.json({
      data: tasks,
      total: tasks.length,
    });
  } catch (error) {
    console.error('Error getting all tasks:', error);
    res.status(500).json({
      error: 'Internal Server Error',
      message: error.message,
    });
  }
};

/**
 * @route   GET /api/scraping/stats
 * @desc    RÃ©cupÃ©rer les statistiques du gestionnaire de tÃ¢ches
 * @access  Public
 */
export const getScrapingStats = async (req, res) => {
  try {
    const stats = taskManager.getStats();

    res.json(stats);
  } catch (error) {
    console.error('Error getting scraping stats:', error);
    res.status(500).json({
      error: 'Internal Server Error',
      message: error.message,
    });
  }
};
