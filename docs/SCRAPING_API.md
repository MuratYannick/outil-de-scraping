# üì° API de Scraping - Documentation

**Derni√®re mise √† jour** : 27 novembre 2025

Ce document d√©crit l'API de scraping asynchrone qui permet de lancer, suivre et g√©rer des t√¢ches de scraping.

## üÜï Nouveaut√©s (Jour 25)

### Pages Jaunes - Am√©liorations majeures

‚úÖ **Extraction optimale des donn√©es** :
- Mise √† jour des s√©lecteurs DOM pour 2024 (`.bi-list > li`, `.bi-denomination h3`)
- Extraction des num√©ros de t√©l√©phone depuis `.bi-fantomas .number-contact`
- Nettoyage automatique des adresses ("Voir le plan", "Site web")
- Extraction automatique du code postal et de la ville dans des champs s√©par√©s

‚úÖ **Nouvelles fonctionnalit√©s** :
- M√©thode `extractAddressComponents()` pour parser les adresses fran√ßaises
- S√©paration automatique : adresse / code postal / ville
- Normalisation des t√©l√©phones au format fran√ßais
- **Option `excludeDuplicates`** : Scraper jusqu'√† N nouveaux prospects (hors doublons)
  - V√©rification en temps r√©el contre la base de donn√©es
  - Continue jusqu'√† obtenir le nombre demand√© de NOUVEAUX prospects
  - M√©triques d√©taill√©es : `duplicates_skipped`, `total_scraped`

‚úÖ **Correction de bugs** :
- Fix `ReferenceError: result is not defined` dans scrapingController
- Meilleure gestion du scope des variables

**Exemple de donn√©es extraites** :
```json
{
  "nom_entreprise": "Artisans Bernard Et Sylvestre",
  "telephone": "01 44 40 02 61",
  "adresse": "7 rue Rochebrune",
  "ville": "Paris",
  "code_postal": "75011",
  "url_site": null,
  "source_scraping": "Pages Jaunes"
}
```

---

## üìã Table des Mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [Endpoints API](#endpoints-api)
- [Workflow](#workflow)
- [Tests](#tests)
- [Utilisation](#utilisation)

---

## Vue d'ensemble

L'API de scraping permet de :
- ‚úÖ Lancer des t√¢ches de scraping de mani√®re asynchrone
- ‚úÖ Suivre la progression en temps r√©el
- ‚úÖ Sauvegarder automatiquement les prospects en base de donn√©es
- ‚úÖ G√©rer les doublons automatiquement
- ‚úÖ Annuler des t√¢ches en cours
- ‚úÖ R√©cup√©rer l'historique des t√¢ches

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client     ‚îÇ
‚îÇ  (Frontend)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ POST /api/scraping/lancer
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ScrapingController   ‚îÇ
‚îÇ  - Validation        ‚îÇ
‚îÇ  - Cr√©ation t√¢che    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TaskManager        ‚îÇ
‚îÇ  - Gestion √©tat      ‚îÇ
‚îÇ  - √âv√©nements        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PagesJaunesScraper   ‚îÇ
‚îÇ  - Scraping async    ‚îÇ
‚îÇ  - Callbacks progr√®s ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sauvegarde DB       ‚îÇ
‚îÇ  - Prospects         ‚îÇ
‚îÇ  - Tags              ‚îÇ
‚îÇ  - Doublons          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Endpoints API

### POST /api/scraping/lancer

Lance une nouvelle t√¢che de scraping.

**Body** :
```json
{
  "keyword": "plombier",
  "location": "Lyon",
  "source": "Pages Jaunes",      // Optionnel, d√©faut: "Pages Jaunes"
  "maxPages": 1,                 // Optionnel, d√©faut: 1
  "maxResults": 10,              // Optionnel, d√©faut: 10
  "excludeDuplicates": false     // Optionnel, d√©faut: false (voir ci-dessous)
}
```

**‚öôÔ∏è Param√®tre `excludeDuplicates`** :

- `false` (d√©faut) : Le scraper s'arr√™te apr√®s avoir scrap√© `maxResults` prospects (peut inclure des doublons)
- `true` : Le scraper continue jusqu'√† trouver `maxResults` NOUVEAUX prospects (exclut les doublons d√©j√† en DB)

**Exemple d'utilisation** :

Si vous avez d√©j√† 20 restaurants √† Cannes en base de donn√©es et que vous lancez un scraping avec `maxResults: 10` et `excludeDuplicates: true`, le scraper va :
1. V√©rifier chaque prospect contre la base de donn√©es en temps r√©el
2. Ignorer les doublons et continuer √† scraper
3. S'arr√™ter uniquement quand 10 NOUVEAUX restaurants ont √©t√© trouv√©s

Sans `excludeDuplicates`, le scraper s'arr√™terait apr√®s avoir scrap√© 10 prospects au total (m√™me si tous sont des doublons).

**Response** (202 Accepted) :
```json
{
  "task_id": "uuid-de-la-tache",
  "status": "pending",
  "message": "T√¢che de scraping cr√©√©e et lanc√©e",
  "params": {
    "keyword": "plombier",
    "location": "Lyon",
    "source": "Pages Jaunes",
    "maxPages": 1,
    "maxResults": 10
  }
}
```

---

### GET /api/scraping/status/:task_id

R√©cup√®re le statut et la progression d'une t√¢che.

**Response** (200 OK) :
```json
{
  "task_id": "uuid-de-la-tache",
  "status": "in_progress",  // pending, in_progress, completed, failed, cancelled
  "progress": 45,            // 0-100
  "params": {
    "keyword": "plombier",
    "location": "Lyon"
  },
  "results": {
    "total": 5,
    "pages_scraped": 1,
    "errors": [],
    "duplicates_skipped": 3,      // Si excludeDuplicates=true
    "total_scraped": 8            // Si excludeDuplicates=true (total + duplicates)
  },
  "createdAt": "2025-11-17T10:30:00Z",
  "startedAt": "2025-11-17T10:30:05Z",
  "completedAt": null,
  "error": null
}
```

**√âtats possibles** :
- `pending` : T√¢che cr√©√©e, en attente de d√©marrage
- `in_progress` : Scraping en cours
- `completed` : Termin√© avec succ√®s
- `failed` : √âchou√© (voir le champ `error`)
- `cancelled` : Annul√© par l'utilisateur

---

### POST /api/scraping/cancel/:task_id

Annule une t√¢che en cours.

**Response** (200 OK) :
```json
{
  "task_id": "uuid-de-la-tache",
  "status": "cancelled",
  "message": "T√¢che annul√©e avec succ√®s"
}
```

**Response** (400 Bad Request) :
```json
{
  "error": "Bad Request",
  "message": "Impossible d'annuler cette t√¢che (non trouv√©e ou d√©j√† termin√©e)"
}
```

---

### GET /api/scraping/tasks

R√©cup√®re toutes les t√¢ches (avec filtres optionnels).

**Query Parameters** :
- `status` : Filtrer par statut (pending, in_progress, completed, failed, cancelled)
- `limit` : Nombre maximum de r√©sultats (d√©faut: 20)

**Exemple** :
```
GET /api/scraping/tasks?status=completed&limit=10
```

**Response** (200 OK) :
```json
{
  "data": [
    {
      "id": "uuid-1",
      "status": "completed",
      "progress": 100,
      "results": {
        "total": 10,
        "pages_scraped": 2
      },
      "createdAt": "2025-11-17T10:00:00Z"
    }
  ],
  "total": 1
}
```

---

### GET /api/scraping/stats

R√©cup√®re les statistiques du gestionnaire de t√¢ches.

**Response** (200 OK) :
```json
{
  "total": 15,
  "pending": 0,
  "in_progress": 2,
  "completed": 10,
  "failed": 2,
  "cancelled": 1
}
```

---

## Workflow

### 1. Lancer un scraping

```javascript
// Frontend
const response = await axios.post('/api/scraping/lancer', {
  keyword: 'plombier',
  location: 'Lyon',
  maxPages: 2,
  maxResults: 20
});

const taskId = response.data.task_id;
```

### 2. Suivre la progression (polling)

```javascript
// Polling toutes les 2 secondes
const interval = setInterval(async () => {
  const status = await axios.get(`/api/scraping/status/${taskId}`);

  console.log(`Progress: ${status.data.progress}%`);
  console.log(`Prospects: ${status.data.results.total}`);

  if (status.data.status === 'completed') {
    clearInterval(interval);
    console.log('Scraping termin√© !');
  }

  if (status.data.status === 'failed') {
    clearInterval(interval);
    console.error('Erreur:', status.data.error);
  }
}, 2000);
```

### 3. R√©cup√©rer les prospects

Une fois la t√¢che termin√©e, les prospects sont automatiquement sauvegard√©s en DB.

```javascript
// R√©cup√©rer les prospects sauvegard√©s
const prospects = await axios.get('/api/prospects?source=Pages Jaunes');
```

---

## Tests

### Test du TaskManager

```bash
cd backend
node scripts/test-task-manager.js
```

**R√©sultat attendu** : 12/12 tests pass√©s ‚úÖ

### Test de l'int√©gration compl√®te

```bash
# Terminal 1: D√©marrer le serveur backend
cd backend
npm run dev

# Terminal 2: Lancer les tests
cd backend
node scripts/test-scraping-api.js
```

**Tests effectu√©s** :
1. ‚úÖ Health check du serveur
2. ‚úÖ Lancement d'une t√¢che de scraping
3. ‚úÖ R√©cup√©ration du statut
4. ‚úÖ Suivi de la progression jusqu'√† completion
5. ‚úÖ R√©cup√©ration de toutes les t√¢ches
6. ‚úÖ Statistiques du gestionnaire
7. ‚úÖ V√©rification des prospects en DB

---

## Utilisation

### Configuration Anti-Bot

L'API de scraping utilise automatiquement la configuration anti-bot d√©finie dans `.env` :

```env
# Strat√©gie anti-bot (none, proxies, captcha_solver, stealth, hybrid)
ANTIBOT_STRATEGY=stealth

# Proxies (si ANTIBOT_STRATEGY=proxies)
PROXY_ENABLED=true
PROXY_PROVIDER=smartproxy

# CAPTCHA Solver (si ANTIBOT_STRATEGY=captcha_solver)
CAPTCHA_SOLVER_ENABLED=true
CAPTCHA_SOLVER_PROVIDER=2captcha

# Stealth Mode (si ANTIBOT_STRATEGY=stealth)
STEALTH_ENABLED=true
```

Voir [ANTIBOT_CONFIG.md](./ANTIBOT_CONFIG.md) pour plus de d√©tails.

### Gestion des Doublons

Les doublons sont automatiquement d√©tect√©s et ignor√©s lors de la sauvegarde :

**Crit√®res de d√©tection** :
- Email identique
- OU URL de site identique

**Comportement** :
- Si un doublon est d√©tect√©, le prospect est ignor√©
- Un log est affich√© : `[ScrapingController] Doublon d√©tect√©: [nom]`
- Le compteur `duplicates_skipped` est incr√©ment√©

### Tags Automatiques

Un tag est automatiquement cr√©√© et associ√© √† chaque prospect bas√© sur le mot-cl√© de recherche :

```javascript
// Recherche: "plombier" ‚Üí Tag: "Plombier"
// Recherche: "restaurant" ‚Üí Tag: "Restaurant"
```

---

## Limitations

### Actuelles

1. **Stockage en m√©moire** : Les t√¢ches sont stock√©es en m√©moire (max 100)
   - En production, utiliser Redis ou une DB pour la persistance

2. **Pas de WebSocket** : Le client doit faire du polling pour la progression
   - Future am√©lioration : WebSocket ou Server-Sent Events

3. **Mono-instance** : Le TaskManager est un singleton
   - Pour du scaling horizontal, utiliser un gestionnaire de t√¢ches distribu√© (Bull, etc.)

### Anti-Bot

Le scraping peut √™tre bloqu√© par Pages Jaunes selon la strat√©gie utilis√©e :

| Strat√©gie | Efficacit√© | Co√ªt |
|-----------|-----------|------|
| **none** | ‚ùå Bloqu√© | Gratuit |
| **stealth** | ‚ö†Ô∏è Variable | Gratuit |
| **proxies** | ‚úÖ‚úÖ‚úÖ Excellent | $75-$1000/mois |
| **captcha_solver** | ‚úÖ‚úÖ Bon | $0.15-$3/1000p |
| **hybrid** | ‚úÖ‚úÖ‚úÖ‚úÖ Maximum | $75-$1003/mois |

**Recommandation** : Commencer avec `stealth` (gratuit) puis ajouter `captcha_solver` si n√©cessaire.

---

## Prochaines Am√©liorations

### Court terme
- [ ] Ajouter la validation Joi sur les routes
- [ ] Impl√©menter le rate limiting
- [ ] Ajouter des logs structur√©s (Winston)

### Moyen terme
- [ ] WebSocket pour feedback temps r√©el (pas de polling)
- [ ] Persistance des t√¢ches en DB ou Redis
- [ ] Interface de monitoring (dashboard admin)

### Long terme
- [ ] Gestion de t√¢ches distribu√©es (Bull/BullMQ)
- [ ] Scraping de sources multiples (Google Maps, LinkedIn)
- [ ] Syst√®me de priorit√©s et de file d'attente

---

## D√©pannage

### Erreur: "Impossible de joindre le serveur"

**Solution** : V√©rifier que le backend est d√©marr√©
```bash
cd backend && npm run dev
```

### Erreur: "Timeout (t√¢che trop longue)"

**Causes possibles** :
1. Site web lent ou indisponible
2. Anti-bot bloque le scraping
3. Trop de pages demand√©es

**Solutions** :
- R√©duire `maxPages` et `maxResults`
- Activer une strat√©gie anti-bot (voir [ANTIBOT_CONFIG.md](./ANTIBOT_CONFIG.md))

### T√¢che bloqu√©e en "in_progress"

**Cause** : Erreur non g√©r√©e dans le scraper

**Solution** : V√©rifier les logs du backend pour identifier l'erreur

---

**Questions ?** Voir aussi :
- [API.md](./API.md) - Sp√©cifications compl√®tes de l'API
- [ANTIBOT_CONFIG.md](./ANTIBOT_CONFIG.md) - Configuration anti-bot
- [TESTS.md](./TESTS.md) - Documentation des tests

---

**Derni√®re mise √† jour** : 27 novembre 2025
**Version** : 1.2.0 (Jour 25 - Optimisation Pages Jaunes)
