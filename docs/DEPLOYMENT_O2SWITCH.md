# üöÄ Guide de D√©ploiement sur O2Switch

**Date** : 10 d√©cembre 2025
**Version** : MVP 1.0

Ce guide d√©taille le d√©ploiement complet de l'outil de scraping sur O2Switch.

---

## üìã Pr√©requis

### Compte O2Switch
- ‚úÖ Compte O2Switch actif
- ‚úÖ Acc√®s cPanel
- ‚úÖ Acc√®s SSH activ√©
- ‚úÖ Nom de domaine configur√© (ou sous-domaine)

### Logiciels Locaux
- Node.js >= 22.19.0
- Git
- Client SSH (PuTTY ou terminal)

---

## üèóÔ∏è Architecture sur O2Switch

```
O2Switch H√©bergement
‚îú‚îÄ‚îÄ Frontend (React)
‚îÇ   ‚îî‚îÄ‚îÄ /public_html/scraping-tool/
‚îÇ       ‚îî‚îÄ‚îÄ Build React statique
‚îÇ
‚îú‚îÄ‚îÄ Backend (Node.js)
‚îÇ   ‚îî‚îÄ‚îÄ /nodejs/scraping-api/
‚îÇ       ‚îú‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ node_modules/
‚îÇ       ‚îú‚îÄ‚îÄ .env.production
‚îÇ       ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îî‚îÄ‚îÄ Base de Donn√©es (MySQL)
    ‚îî‚îÄ‚îÄ outil_scraping_db
        ‚îú‚îÄ‚îÄ prospects
        ‚îú‚îÄ‚îÄ tags
        ‚îú‚îÄ‚îÄ sources_scraping
        ‚îî‚îÄ‚îÄ tables de liaison
```

---

## üì¶ √âtape 1 : Pr√©paration Locale

### 1.1 Build du Frontend

```bash
cd frontend
npm install
npm run build
```

**R√©sultat** : Dossier `frontend/dist/` avec les fichiers statiques optimis√©s

### 1.2 Pr√©paration du Backend

```bash
cd backend
npm install --production
```

**Note** : Installer uniquement les d√©pendances de production (sans devDependencies)

### 1.3 Configuration .env.production

Cr√©er `backend/.env.production` :

```env
# Environment
NODE_ENV=production

# Database
DB_HOST=localhost
DB_PORT=3306
DB_NAME=votrecompte_outil_scraping
DB_USER=votrecompte_outil_user
DB_PASSWORD=VOTRE_MOT_DE_PASSE_SECURISE

# Server
PORT=3001
FRONTEND_URL=https://votre-domaine.com

# Anti-Bot Configuration
ANTIBOT_STRATEGY=stealth
STEALTH_ENABLED=true
BROWSER_PROFILE_PATH=./browser-profiles/production

# Playwright
PLAYWRIGHT_BROWSERS_PATH=/home/votrecompte/nodejs/scraping-api/browsers
```

‚ö†Ô∏è **Important** : Remplacer tous les `votrecompte` et `votre-domaine.com` par vos valeurs r√©elles

---

## üóÑÔ∏è √âtape 2 : Configuration Base de Donn√©es MySQL

### 2.1 Cr√©er la Base de Donn√©es via cPanel

1. **Connexion cPanel** : https://votre-domaine.com:2083
2. **MySQL¬Æ Databases**
3. **Cr√©er une nouvelle base de donn√©es** :
   - Nom : `outil_scraping`
   - cPanel ajoutera automatiquement le pr√©fixe : `votrecompte_outil_scraping`

### 2.2 Cr√©er un Utilisateur MySQL

1. **Section "Utilisateurs MySQL"**
2. **Cr√©er un nouvel utilisateur** :
   - Nom d'utilisateur : `outil_user`
   - Mot de passe : G√©n√©rer un mot de passe fort
   - Pr√©fixe auto : `votrecompte_outil_user`

### 2.3 Associer l'Utilisateur √† la Base de Donn√©es

1. **Section "Ajouter un utilisateur √† une base de donn√©es"**
2. S√©lectionner :
   - Utilisateur : `votrecompte_outil_user`
   - Base de donn√©es : `votrecompte_outil_scraping`
3. **Privil√®ges** : Cocher "TOUS LES PRIVIL√àGES"

### 2.4 Importer le Sch√©ma de Base de Donn√©es

**Option A : Via phpMyAdmin (recommand√©)**

1. Ouvrir **phpMyAdmin** depuis cPanel
2. S√©lectionner la base `votrecompte_outil_scraping`
3. Onglet **"SQL"**
4. Copier-coller le sch√©ma suivant :

```sql
-- Table prospects
CREATE TABLE IF NOT EXISTS `prospects` (
  `id` INT AUTO_INCREMENT PRIMARY KEY,
  `nom_entreprise` VARCHAR(255) NOT NULL,
  `nom_contact` VARCHAR(255) DEFAULT NULL,
  `email` VARCHAR(255) DEFAULT NULL,
  `telephone` VARCHAR(50) DEFAULT NULL,
  `telephone_2` VARCHAR(50) DEFAULT NULL,
  `telephone_3` VARCHAR(50) DEFAULT NULL,
  `adresse` TEXT DEFAULT NULL,
  `url_site` VARCHAR(500) DEFAULT NULL,
  `latitude` DECIMAL(10,8) DEFAULT NULL,
  `longitude` DECIMAL(11,8) DEFAULT NULL,
  `note` TEXT DEFAULT NULL,
  `ville` VARCHAR(255) DEFAULT NULL,
  `code_postal` VARCHAR(10) DEFAULT NULL,
  `date_ajout` DATETIME DEFAULT CURRENT_TIMESTAMP,
  `date_modification` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  INDEX `idx_nom_entreprise` (`nom_entreprise`),
  INDEX `idx_ville` (`ville`),
  INDEX `idx_date_ajout` (`date_ajout`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- Table tags
CREATE TABLE IF NOT EXISTS `tags` (
  `id` INT AUTO_INCREMENT PRIMARY KEY,
  `nom` VARCHAR(100) NOT NULL UNIQUE,
  INDEX `idx_nom` (`nom`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- Table sources_scraping
CREATE TABLE IF NOT EXISTS `sources_scraping` (
  `id` INT AUTO_INCREMENT PRIMARY KEY,
  `nom` VARCHAR(100) NOT NULL UNIQUE,
  `description` TEXT DEFAULT NULL,
  `couleur` VARCHAR(7) DEFAULT '#3B82F6',
  `actif` BOOLEAN DEFAULT TRUE,
  `date_creation` DATETIME DEFAULT CURRENT_TIMESTAMP,
  INDEX `idx_nom` (`nom`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- Table de liaison prospects_tags
CREATE TABLE IF NOT EXISTS `prospects_tags` (
  `prospect_id` INT NOT NULL,
  `tag_id` INT NOT NULL,
  `created_at` DATETIME DEFAULT CURRENT_TIMESTAMP,
  `updated_at` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`prospect_id`, `tag_id`),
  FOREIGN KEY (`prospect_id`) REFERENCES `prospects`(`id`) ON DELETE CASCADE,
  FOREIGN KEY (`tag_id`) REFERENCES `tags`(`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- Table de liaison prospects_sources
CREATE TABLE IF NOT EXISTS `prospects_sources` (
  `prospect_id` INT NOT NULL,
  `source_id` INT NOT NULL,
  `created_at` DATETIME DEFAULT CURRENT_TIMESTAMP,
  `updated_at` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`prospect_id`, `source_id`),
  FOREIGN KEY (`prospect_id`) REFERENCES `prospects`(`id`) ON DELETE CASCADE,
  FOREIGN KEY (`source_id`) REFERENCES `sources_scraping`(`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- Donn√©es initiales : Sources
INSERT INTO `sources_scraping` (`nom`, `description`, `couleur`) VALUES
('Pages Jaunes', 'Annuaire Pages Jaunes', '#FFD700'),
('Google Maps', 'Google Maps Business', '#4285F4')
ON DUPLICATE KEY UPDATE nom=nom;

-- Donn√©es initiales : Tags par d√©faut
INSERT INTO `tags` (`nom`) VALUES
('Restaurant'),
('Commerce'),
('Service'),
('Artisan'),
('Professionnel'),
('Particulier'),
('√Ä contacter'),
('Prioritaire')
ON DUPLICATE KEY UPDATE nom=nom;
```

5. Cliquer sur **"Ex√©cuter"**

**Option B : Via SSH**

```bash
mysql -u votrecompte_outil_user -p votrecompte_outil_scraping < schema.sql
```

---

## üì§ √âtape 3 : Upload des Fichiers

### 3.1 Upload du Frontend

**Via FTP/SFTP (FileZilla, WinSCP...)**

1. Connexion :
   - H√¥te : `ftp.votre-domaine.com` ou `ssh.o2switch.net`
   - Utilisateur : Votre compte cPanel
   - Mot de passe : Mot de passe cPanel
   - Port : 21 (FTP) ou 22 (SFTP)

2. Naviguer vers `/public_html/`

3. Cr√©er le dossier `scraping-tool/`

4. Uploader **TOUT le contenu** du dossier `frontend/dist/` vers `/public_html/scraping-tool/`

**Structure finale** :
```
/public_html/scraping-tool/
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ index-xxx.js
‚îÇ   ‚îî‚îÄ‚îÄ index-xxx.css
‚îî‚îÄ‚îÄ ...
```

### 3.2 Upload du Backend

**Via SFTP ou SSH**

1. Cr√©er le dossier backend :
```bash
mkdir -p /home/votrecompte/nodejs/scraping-api
```

2. Uploader les fichiers suivants :
   - `backend/src/` (tout le dossier)
   - `backend/package.json`
   - `backend/package-lock.json`
   - `backend/.env.production` ‚Üí renommer en `.env`

**Ne PAS uploader** :
- ‚ùå `node_modules/` (sera install√© sur le serveur)
- ‚ùå `.env` de d√©veloppement
- ‚ùå `scripts/` (tests uniquement)

---

## ‚öôÔ∏è √âtape 4 : Configuration Node.js sur O2Switch

### 4.1 Activer Node.js via cPanel

1. **Setup Node.js App** dans cPanel
2. **Create Application** :
   - Node.js version : **22.x** (derni√®re stable)
   - Application mode : **Production**
   - Application root : `/home/votrecompte/nodejs/scraping-api`
   - Application URL : `scraping-api.votre-domaine.com` (ou sous-domaine)
   - Application startup file : `src/app.js`

3. Cliquer sur **"Create"**

### 4.2 Installer les D√©pendances

1. Cliquer sur **"Run NPM Install"** dans l'interface cPanel

**OU** via SSH :

```bash
cd /home/votrecompte/nodejs/scraping-api
source /home/votrecompte/nodevenv/scraping-api/22/bin/activate
npm install --production
```

### 4.3 Installer Playwright et les Navigateurs

```bash
cd /home/votrecompte/nodejs/scraping-api
source /home/votrecompte/nodevenv/scraping-api/22/bin/activate

# Installer Playwright
npm install playwright

# Installer les navigateurs (Chromium seulement pour √©conomiser l'espace)
npx playwright install chromium
```

‚ö†Ô∏è **Note** : L'installation des navigateurs peut prendre 5-10 minutes et ~300 MB d'espace disque.

### 4.4 D√©marrer l'Application

1. Dans cPanel ‚Üí **Setup Node.js App**
2. Cliquer sur votre application
3. Cliquer sur **"Restart"** ou **"Start"**

**V√©rification** :
```bash
curl https://scraping-api.votre-domaine.com/health
```

**R√©sultat attendu** :
```json
{
  "status": "ok",
  "message": "API Scraping is running",
  "timestamp": "2025-12-10T..."
}
```

---

## üîß √âtape 5 : Configuration Frontend pour l'API Production

### 5.1 Mettre √† Jour l'URL de l'API

**Fichier** : `frontend/.env.production` (cr√©er s'il n'existe pas)

```env
VITE_API_URL=https://scraping-api.votre-domaine.com
```

### 5.2 Rebuild et Re-upload

```bash
cd frontend
npm run build
```

Re-uploader le contenu de `dist/` vers `/public_html/scraping-tool/`

---

## üîí √âtape 6 : S√©curit√© et SSL

### 6.1 Activer SSL (Let's Encrypt)

1. **cPanel** ‚Üí **SSL/TLS Status**
2. Cocher votre domaine et sous-domaine API
3. Cliquer sur **"Run AutoSSL"**

**R√©sultat** : Certificats SSL install√©s automatiquement et gratuits

### 6.2 Configuration CORS

Le backend est d√©j√† configur√© pour accepter les requ√™tes du frontend en production.

**V√©rifier** : `backend/src/app.js`

```javascript
const corsOptions = {
  origin: process.env.FRONTEND_URL || 'http://localhost:5173',
  credentials: true,
};
```

---

## üß™ √âtape 7 : Tests Post-D√©ploiement

### 7.1 Test du Frontend

1. Ouvrir : `https://votre-domaine.com/scraping-tool/`
2. V√©rifier :
   - ‚úÖ Page se charge correctement
   - ‚úÖ Aucune erreur dans la console (F12)
   - ‚úÖ Connexion API (point vert dans le header)

### 7.2 Test de l'API Backend

```bash
# Health check
curl https://scraping-api.votre-domaine.com/health

# Liste des prospects (devrait retourner un tableau vide au d√©part)
curl https://scraping-api.votre-domaine.com/api/prospects
```

### 7.3 Test de Scraping Complet

1. Aller sur l'onglet **"Scraping"**
2. Remplir le formulaire :
   - Source : **Pages Jaunes**
   - Mot-cl√© : `restaurant`
   - Localisation : `Paris`
   - Max r√©sultats : `5`
3. Cliquer sur **"Lancer le scraping"**
4. V√©rifier :
   - ‚úÖ T√¢che lanc√©e avec succ√®s
   - ‚úÖ Progression visible
   - ‚úÖ Prospects sauvegard√©s dans la base de donn√©es

---

## üìä Monitoring et Logs

### Via cPanel

1. **Setup Node.js App** ‚Üí Votre application
2. Cliquer sur **"Open logs"**

### Via SSH

```bash
# Logs en temps r√©el
tail -f /home/votrecompte/nodejs/scraping-api/logs/app.log

# Logs d'erreur Node.js
tail -f /home/votrecompte/logs/scraping-api-error.log
```

---

## üîÑ Mises √† Jour Futures

### Mise √† Jour du Frontend

```bash
cd frontend
git pull
npm install
npm run build
# Re-upload dist/ vers /public_html/scraping-tool/
```

### Mise √† Jour du Backend

```bash
# Sur le serveur
cd /home/votrecompte/nodejs/scraping-api
git pull
source /home/votrecompte/nodevenv/scraping-api/22/bin/activate
npm install --production

# Red√©marrer l'app via cPanel ‚Üí Setup Node.js App ‚Üí Restart
```

---

## üÜò D√©pannage

### Probl√®me : Backend ne d√©marre pas

**Solution** :
1. V√©rifier les logs : cPanel ‚Üí Setup Node.js App ‚Üí Open logs
2. V√©rifier `.env` : Les credentials MySQL sont corrects ?
3. Test de connexion DB :
```bash
mysql -u votrecompte_outil_user -p votrecompte_outil_scraping -e "SELECT 1"
```

### Probl√®me : Playwright ne trouve pas les navigateurs

**Solution** :
```bash
cd /home/votrecompte/nodejs/scraping-api
source /home/votrecompte/nodevenv/scraping-api/22/bin/activate
PLAYWRIGHT_BROWSERS_PATH=/home/votrecompte/nodejs/scraping-api/browsers npx playwright install chromium
```

### Probl√®me : CORS errors

**Solution** :
1. V√©rifier `FRONTEND_URL` dans `.env` :
```env
FRONTEND_URL=https://votre-domaine.com
```
2. Red√©marrer le backend

### Probl√®me : Frontend ne se connecte pas √† l'API

**Solution** :
1. V√©rifier `frontend/.env.production` :
```env
VITE_API_URL=https://scraping-api.votre-domaine.com
```
2. Rebuild le frontend
3. Re-upload

---

## üìû Support O2Switch

**Si probl√®me technique** :
- Email : support@o2switch.fr
- Ticket : Depuis cPanel ‚Üí Support
- T√©l√©phone : +33 4 44 44 60 40

---

## ‚úÖ Checklist Finale

Avant de consid√©rer le d√©ploiement termin√© :

- [ ] Base de donn√©es MySQL cr√©√©e et sch√©ma import√©
- [ ] Backend upload√© et Node.js configur√©
- [ ] D√©pendances install√©es (`npm install`)
- [ ] Playwright et Chromium install√©s
- [ ] Backend d√©marr√© et accessible (`/health` retourne OK)
- [ ] Frontend build√© et upload√©
- [ ] SSL activ√© sur domaine et sous-domaine API
- [ ] Test de scraping r√©ussi (5 prospects Pages Jaunes)
- [ ] Prospects visibles dans l'onglet "Prospects"
- [ ] Export CSV fonctionne
- [ ] Gestion des tags fonctionne

---

**Date de d√©ploiement** : _____________
**D√©ploy√© par** : _____________
**Version** : MVP 1.0
**URL Production** : https://votre-domaine.com/scraping-tool/

üéâ **F√©licitations ! Votre outil de scraping est maintenant en production sur O2Switch !**
