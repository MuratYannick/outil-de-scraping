# ğŸ“Š Progression du Projet Outil de Scraping

**DerniÃ¨re mise Ã  jour** : 13 novembre 2025

## ğŸ¯ Objectif Phase 1 (MVP)

- [x] DÃ©finir les objectifs du MVP
  - Collecter 50 prospects initialement
  - Ã‰tablir un flux rÃ©current de 10 prospects/semaine
  - Stocker et visualiser les donnÃ©es collectÃ©es

**DurÃ©e prÃ©vue** : 4 semaines (20 jours de dÃ©veloppement)

---

## ğŸ“… Roadmap et Statut

### Semaine 1 : ğŸ—ï¸ Infrastructure Backend & DB (âœ… COMPLÃ‰TÃ‰E Ã  100%)

#### Jour 1 : Initialisation du projet & Architecture
- [x] CrÃ©er le repository GitHub
- [x] Initialiser la structure du projet (backend/frontend/docs)
- [x] Valider la stack technique (Node.js 22.19.0, Express, Sequelize, MySQL, Vite, React)
- [x] Configurer Git et .gitignore

#### Jour 2-3 : Configuration de la base de donnÃ©es
- [x] Installer et configurer MySQL localement
- [x] CrÃ©er le schÃ©ma de base de donnÃ©es (prospects, tags, prospects_tags)
- [x] CrÃ©er les modÃ¨les Sequelize (Prospect, Tag)
- [x] ImplÃ©menter les associations N:M entre Prospect et Tag
- [x] CrÃ©er le script SQL d'initialisation (`init-db.sql`)
- [x] CrÃ©er les scripts de gestion DB (`setup-db.js`, `migrate.js`)
- [x] CrÃ©er le script de suppression des tables (`drop-tables.js`)
- [x] CrÃ©er le script de seed avec donnÃ©es de test (`seed-db.js`)

#### Jour 4 : API de gestion des donnÃ©es (CRUD)
- [x] Configurer Express app minimale
- [x] Configurer la connexion MySQL avec Sequelize
- [x] CrÃ©er la route `/health` pour vÃ©rification serveur
- [x] CrÃ©er les controllers (prospectController.js, tagController.js)
- [x] CrÃ©er les routes `/api/prospects` (GET, POST, PUT, DELETE + tags)
- [x] CrÃ©er les routes `/api/tags` (GET, POST, PUT, DELETE)
- [x] Tester les endpoints API

#### Jour 5 : Initialisation du Frontend & connexion API
- [x] Initialiser Vite 7.x avec React 18
- [x] Configurer Tailwind CSS v3
- [x] CrÃ©er la structure de base (main.jsx, App.jsx)
- [x] Configurer PostCSS et autoprefixer
- [x] Mettre Ã  niveau Vite 5.x â†’ 7.x (rÃ©soudre advisory esbuild)
- [x] CrÃ©er le service API avec Axios (api.js)
- [x] CrÃ©er les composants de base React (Header, ProspectList)
- [x] Connecter le frontend Ã  l'API backend
- [x] Tester la communication frontend/backend

---

### Semaine 2 : ğŸ•·ï¸ Moteur de Scraping MVP (ğŸ“‹ Ã€ FAIRE)

#### Jour 6 : Mise en place de Playwright
- [ ] Installer Playwright et ses dÃ©pendances
- [ ] CrÃ©er le service `playwrightService.js`
- [ ] ImplÃ©menter les utilitaires de base (pool de contexts, retry, logging)
- [ ] Configurer l'Ã©mulation de navigateur (User-Agent, viewport)
- [ ] Tester le lancement basique de Playwright

#### Jour 7-8 : DÃ©veloppement du scraper Pages Jaunes
- [ ] Analyser la structure HTML de Pages Jaunes
- [ ] CrÃ©er le scraper `pagesJaunesScraper.js`
- [ ] ImplÃ©menter l'extraction des donnÃ©es (nom, adresse, tÃ©lÃ©phone, site web)
- [ ] Ajouter la normalisation des donnÃ©es (format tÃ©lÃ©phone, emails)
- [ ] ImplÃ©menter la gestion des erreurs et retry
- [ ] Ajouter la logique anti-dÃ©tection (delays, rotation proxies si disponible)
- [ ] Tester le scraper avec plusieurs requÃªtes

#### Jour 9 : IntÃ©gration du scraper Ã  l'API
- [ ] CrÃ©er les routes `/api/scraping/lancer` et `/api/scraping/status/:task_id`
- [ ] CrÃ©er le controller de scraping
- [ ] ImplÃ©menter la gestion des tÃ¢ches asynchrones
- [ ] Ajouter le feedback en temps rÃ©el (progression, nombre de prospects)
- [ ] Tester l'intÃ©gration API â†” Scraper

#### Jour 10 : Sauvegarde des donnÃ©es & feedback
- [ ] ImplÃ©menter la sauvegarde automatique des prospects en DB
- [ ] Ajouter la dÃ©tection et gestion des doublons
- [ ] CrÃ©er le systÃ¨me de feedback utilisateur (notifications, logs)
- [ ] Tester le flux complet : lancement â†’ scraping â†’ sauvegarde â†’ feedback
- [ ] Valider l'objectif de 50 prospects initiaux

---

### Semaine 3 : ğŸ’» Interface Utilisateur (ğŸ“‹ Ã€ FAIRE)

#### Jour 11-12 : Interface de lancement du scraping
- [ ] CrÃ©er le composant formulaire de scraping (keyword, location, source)
- [ ] ImplÃ©menter la validation des inputs cÃ´tÃ© client
- [ ] CrÃ©er le composant d'affichage de progression en temps rÃ©el
- [ ] Ajouter les notifications de succÃ¨s/erreur
- [ ] Styliser avec Tailwind CSS
- [ ] Tester le lancement de scraping depuis l'interface

#### Jour 13 : Tableau de bord des prospects
- [ ] CrÃ©er le composant tableau de prospects
- [ ] ImplÃ©menter la pagination
- [ ] Ajouter les filtres (par tag, par source, par date)
- [ ] CrÃ©er les composants de visualisation (cartes, statistiques)
- [ ] ImplÃ©menter l'export des donnÃ©es (CSV, JSON)
- [ ] Tester l'affichage de donnÃ©es volumineuses

#### Jour 14 : Gestion des tags
- [ ] CrÃ©er le composant de gestion des tags
- [ ] ImplÃ©menter l'ajout/suppression de tags
- [ ] CrÃ©er l'interface d'association prospect â†” tag
- [ ] Ajouter la recherche et filtrage par tags
- [ ] Tester les opÃ©rations CRUD sur les tags

#### Jour 15 : Gestion des erreurs & logique proxy
- [ ] ImplÃ©menter la gestion globale des erreurs frontend
- [ ] CrÃ©er les pages d'erreur (404, 500)
- [ ] Ajouter la validation Joi cÃ´tÃ© backend
- [ ] ImplÃ©menter la logique de rotation des proxies (si applicable)
- [ ] Tester les scÃ©narios d'erreur et la rÃ©cupÃ©ration

---

### Semaine 4 : ğŸŒ Scraping Dynamique & DÃ©ploiement (ğŸ“‹ Ã€ FAIRE)

#### Jour 16-18 : Scraper Google Maps/LinkedIn
- [ ] Analyser la structure de Google Maps
- [ ] CrÃ©er le scraper `googleMapsScraper.js`
- [ ] Tester et valider le scraper Google Maps
- [ ] Analyser la structure de LinkedIn (si applicable)
- [ ] CrÃ©er le scraper `linkedInScraper.js`
- [ ] ImplÃ©menter les stratÃ©gies anti-dÃ©tection spÃ©cifiques
- [ ] Ajouter la gÃ©olocalisation et extraction de coordonnÃ©es
- [ ] Tester les scrapers additionnels

#### Jour 19 : Nettoyage et finalisation du code
- [ ] Refactoring du code backend
- [ ] Refactoring du code frontend
- [ ] Ajouter les commentaires et documentation inline
- [ ] Optimiser les performances (requÃªtes DB, chargement frontend)
- [ ] ExÃ©cuter les linters (ESLint) et corriger les warnings
- [ ] VÃ©rifier la sÃ©curitÃ© (npm audit, validation inputs)
- [ ] CrÃ©er/mettre Ã  jour les tests unitaires

#### Jour 20 : DÃ©ploiement MVP & dÃ©mo
- [ ] PrÃ©parer l'environnement de production
- [ ] DÃ©ployer la base de donnÃ©es (MySQL en prod)
- [ ] DÃ©ployer le backend
- [ ] DÃ©ployer le frontend
- [ ] Configurer les variables d'environnement prod
- [ ] Tester l'application en production
- [ ] PrÃ©parer la dÃ©mo et documentation utilisateur
- [ ] Livrer le MVP au chef de projet

---

## ğŸ” ProblÃ¨mes RÃ©solus

### Security
- [x] **npm audit (Backend)** : Suppression de Puppeteer effectuÃ©e, seul Playwright est utilisÃ©
- [x] **npm audit (Frontend)** : Mise Ã  jour de Vite 5.x â†’ 7.x, rÃ©solution advisory esbuild (GHSA-67mh-4wv8-2f99), audit finalisÃ© Ã  0 vulnÃ©rabilitÃ©s

---

## ğŸ“¦ Versions Actuelles

### Backend

- **Node.js** : 22.19.0
- **npm** : >= 10.0.0
- **Express** : ^4.18.2
- **Sequelize** : ^6.35.2
- **MySQL2** : ^3.6.5
- **Playwright** : ^1.40.1
- **Cheerio** : ^1.0.0-rc.12
- **Dotenv** : ^16.3.1
- **Helmet** : ^7.1.0
- **Axios** : ^1.6.2
- **Joi** : ^17.11.0
- **UUID** : ^9.0.1
- **ESLint** : ^8.55.0

### Frontend

- **Node.js** : 22.19.0
- **npm** : >= 10.0.0
- **React** : ^18.2.0
- **React DOM** : ^18.2.0
- **React Router DOM** : ^6.20.0
- **Vite** : ^7.2.2 (upgraded from ^5.0.8)
- **@vitejs/plugin-react** : ^5.1.1 (upgraded from ^4.2.1)
- **Tailwind CSS** : ^3.3.6
- **PostCSS** : ^8.4.32
- **Autoprefixer** : ^10.4.16
- **Axios** : ^1.6.2
- **ESLint** : ^8.55.0
- **ESLint Plugin React** : ^7.33.2

### Base de DonnÃ©es

- **MySQL** : >= 8.0 (local)

---

## ğŸ—‚ï¸ Structure Actuelle

```
outil-de-scraping/
â”œâ”€â”€ README.md                   # Vue d'ensemble du projet
â”œâ”€â”€ PROGRESS.md                 # Ce fichier (progression du projet)
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app.js              # Express app
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ database.js     # Config Sequelize
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â”‚   â”œâ”€â”€ Prospect.js
â”‚   â”‚   â”‚   â””â”€â”€ Tag.js
â”‚   â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”‚   â”œâ”€â”€ prospectController.js
â”‚   â”‚   â”‚   â””â”€â”€ tagController.js
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ prospectRoutes.js
â”‚   â”‚   â”‚   â””â”€â”€ tagRoutes.js
â”‚   â”‚   â”œâ”€â”€ middlewares/        # Ã€ implÃ©menter
â”‚   â”‚   â””â”€â”€ services/           # Ã€ implÃ©menter (Playwright service)
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ init-db.sql
â”‚       â”œâ”€â”€ setup-db.js
â”‚       â”œâ”€â”€ migrate.js
â”‚       â”œâ”€â”€ drop-tables.js
â”‚       â””â”€â”€ seed-db.js
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ postcss.config.cjs      # PostCSS config (CommonJS)
â”‚   â”œâ”€â”€ .eslintrc.json
â”‚   â”œâ”€â”€ index.html              # EntrÃ©e Vite
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.jsx            # Point d'entrÃ©e
â”‚       â”œâ”€â”€ App.jsx             # Composant principal
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ Header.jsx
â”‚       â”‚   â””â”€â”€ ProspectList.jsx
â”‚       â”œâ”€â”€ pages/              # Ã€ implÃ©menter
â”‚       â”œâ”€â”€ services/
â”‚       â”‚   â””â”€â”€ api.js          # Service API Axios
â”‚       â””â”€â”€ styles/
â”‚           â””â”€â”€ index.css       # Styles Tailwind
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md                  # SpÃ©cifications API
â”‚   â”œâ”€â”€ DATABASE.md             # SchÃ©ma base de donnÃ©es
â”‚   â”œâ”€â”€ SETUP.md                # Guide d'installation
â”‚   â””â”€â”€ PLAYWRIGHT_DECISION.md  # Rationale du choix Playwright
â””â”€â”€ .gitignore
```

---

## ğŸš€ Prochaines Ã‰tapes (PrioritÃ©)

### Semaine 2 â€” Moteur de Scraping (Ã€ commencer immÃ©diatement)
- [ ] ImplÃ©menter `backend/src/services/playwrightService.js`
- [ ] CrÃ©er un scraper exemple (Pages Jaunes)
- [ ] Ajouter routes API pour lancer le scraping
- [ ] Tester le flux complet de scraping

### Semaine 3 â€” Frontend
- [ ] DÃ©velopper composants React (Dashboard, Formulaire scraping, Liste prospects)
- [ ] IntÃ©grer l'API backend avec Axios
- [ ] Afficher les prospects et permettre de lancer un scraping
- [ ] ImplÃ©menter la gestion des tags

### SÃ©curitÃ© & QualitÃ©
- [ ] Ajouter validation Joi sur les routes
- [ ] Tests unitaires (Ã  dÃ©finir avec le chef de projet)
- [ ] Gestion des erreurs amÃ©liorÃ©e
- [ ] Configuration Helmet pour sÃ©curiser les headers HTTP

---

## ğŸ“ Contact & Ressources

- **CrÃ©ateur** : Yannick Murat
- **Email** : muratyannick.dev@gmail.com
- **GitHub** : https://github.com/MuratYannick/outil-de-scraping

---

## ğŸ“ Notes de DÃ©veloppement

### DÃ©cisions Techniques

- **Playwright** choisi comme moteur de scraping (pas Puppeteer)
- **Vite 7.x** pour build frontend rapide et moderne
- **Sequelize** pour ORM MySQL (sync mode en dev, migrations en prod)
- **Tailwind CSS v3** pour styling utilitaire
- **Pas de Docker** pour le MVP (dÃ©ploiement local/simple)

### Configuration

- Frontend build: `npm run build` gÃ©nÃ¨re `dist/`
- Backend dev: `npm run dev` avec nodemon (`node --watch`)
- DB: script `npm run db:migrate` pour Sequelize sync

---

**DerniÃ¨re mise Ã  jour** : 13 novembre 2025
